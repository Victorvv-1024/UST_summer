{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMakfN9bn0CB"
      },
      "source": [
        "<h1>UST Reddit Projecy</h1></r>\n",
        "<h2>Pipeline</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmIuSrbLeJLQ",
        "outputId": "605d7ac7-293f-4e30-86aa-a059c3745af2"
      },
      "outputs": [],
      "source": [
        "# gives accessibility to my goodle drive to fetch the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7xATcS5n0CD"
      },
      "source": [
        "<h2>Generate dataset</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u1dZSTNTn0CD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuCLqUN4n0CE"
      },
      "source": [
        "<h3>Read the raw data WITHOUT CLEANING</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GbQ4ljtn0CE",
        "outputId": "70950837-9915-41c4-e356-0ef314e3d881"
      },
      "outputs": [],
      "source": [
        "fp = '/Users/victor/Desktop/CS/UST_summer/all_posts_clean_1.csv' # the directory of the raw data file, pipeliend from reddits\n",
        "\n",
        "s_columns = ['id', 'author', 'created_utc', 'num_comments', 'clean_title', 'clean_selftext', 'num_title_emojis', 'num_text_emojis', 'title_length', 'text_length', 'score', 'num_replied_comments', 'num_replies_by_new', 'list_new_repliers', 'num_replies_by_old', 'list_old_repliers']\n",
        "\n",
        "df = pd.read_csv(fp, low_memory=False, encoding='utf-8')[s_columns]\n",
        "\n",
        "df.head() # test if it is read correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNXN3S32n0CE"
      },
      "outputs": [],
      "source": [
        "# df = df[df.created_utc < 1590000000.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDPlszy6n0CF",
        "outputId": "072d52d1-4985-4765-b016-1b8238463724"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "df.groupby('author').cumcount()\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "df[\"num_post\"] = df.groupby('author').cumcount().to_numpy()\n",
        "codes, uniques = pd.factorize(df[\"author\"])\n",
        "df[\"author_codes\"] = codes\n",
        "\n",
        "# df.to_csv('wsb_with_authorcode_num_post.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGy-bVvFn0CF"
      },
      "source": [
        "<h2>for statistical tests ONLY</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlVRJONQn0CF"
      },
      "outputs": [],
      "source": [
        "indices = df.index[(df.created_utc >= 1609459200.0) & (df.created_utc <= 1612137599.0)].tolist()\n",
        "\n",
        "# zeros = np.zeros(df.shape[0])\n",
        "# for i in indices : zeros[i] = 1\n",
        "\n",
        "# df['is_2021Jan'] = zeros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzIWsZ-On0CG",
        "outputId": "ead05f8f-9fcd-444b-e13e-1bbfd9f1862d"
      },
      "outputs": [],
      "source": [
        "number_of_replies = list()\n",
        "\n",
        "index = df.index[(df['clean_selftext'] == 'deleted') | (df['clean_selftext'] == 'removed')].tolist()\n",
        "for i in index:\n",
        "    number_of_replies.append(df['num_comments'].iloc[i])\n",
        "    \n",
        "# len(number_of_replies)\n",
        "\n",
        "nonzero_replies = np.count_nonzero(np.array(number_of_replies))\n",
        "nonzero_replies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF_KjIx9n0CG",
        "outputId": "67d9da3b-dd76-4bb6-cfe7-d0976caada9d"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTzczjq0n0CG"
      },
      "outputs": [],
      "source": [
        "# count the number of deleted and removed comments in each month from 2020-01-01 to 2021-05-01\n",
        "date = datetime(2020,1,1,0)\n",
        "time = []\n",
        "\n",
        "for i in range(17):\n",
        "    count_time = date + relativedelta(months=i)\n",
        "    count_time = count_time.timestamp()\n",
        "    time.append(count_time)\n",
        "\n",
        "time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXA_8IDjn0CH"
      },
      "outputs": [],
      "source": [
        "def deleted_comments(left_bound, right_bound, dataframe):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        left_bound (_type_): _description_\n",
        "        right_bound (_type_): _description_\n",
        "        dataframe (_type_): _description_\n",
        "    \"\"\"\n",
        "    indices = dataframe.index[(dataframe.created_utc >= left_bound) & (dataframe.created_utc <= right_bound)].tolist() # workout the total number of comments\n",
        "    \n",
        "    # print(indices)\n",
        "    if len(indices) == 0:\n",
        "        return 0\n",
        "    \n",
        "    unwanted_comments = 0\n",
        "    for i in indices:\n",
        "        if (dataframe['clean_selftext'].iloc[i] == 'deleted') | (dataframe['clean_selftext'].iloc[i] == 'removed'):\n",
        "            unwanted_comments += 1\n",
        "    \n",
        "    return (unwanted_comments/len(indices))*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym8wftLhn0CH"
      },
      "outputs": [],
      "source": [
        "# workout the percentage of deleted commenets within the time range\n",
        "unwanted_percent = []\n",
        "\n",
        "for i in range (len(time)-1):\n",
        "    left = time[i]\n",
        "    right = time[i+1]\n",
        "    \n",
        "    \n",
        "    unwanted_percent.append(deleted_comments(left,right,df))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WisDbEOGn0CH"
      },
      "outputs": [],
      "source": [
        "def replied_percent(left_bound, right_bound, dataframe):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        left_bound (_type_): _description_\n",
        "        right_bound (_type_): _description_\n",
        "        dataframe (_type_): _description_\n",
        "    \"\"\"\n",
        "    indices = dataframe.index[(dataframe.created_utc >= left_bound) & (dataframe.created_utc <= right_bound)].tolist()\n",
        "    \n",
        "    negative_index = []\n",
        "    negative_positive = 0\n",
        "    \n",
        "    for i in indices:\n",
        "        if (dataframe['clean_selftext'].iloc[i] == 'deleted') | (dataframe['clean_selftext'].iloc[i] == 'removed'):\n",
        "            negative_index.append(i)\n",
        "    \n",
        "    for i in negative_index:\n",
        "        if dataframe.num_comments.iloc[i] > 0:\n",
        "            negative_positive += 1\n",
        "    \n",
        "    negative_positive_rate = negative_positive / len(negative_index)\n",
        "    \n",
        "    positive_index = list(set(indices) - set(negative_index))\n",
        "    positive_positive = 0\n",
        "    \n",
        "    for i in positive_index:\n",
        "        if dataframe.num_comments.iloc[i] > 0:\n",
        "            positive_positive += 1\n",
        "    \n",
        "    positive_positive_rate = positive_positive / len(positive_index)\n",
        "    \n",
        "    return (positive_positive_rate, negative_positive_rate)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y_NdrcGvDhz"
      },
      "outputs": [],
      "source": [
        "# workout the percentage of undeleted commenets got replied and deleted comments got replied within the time range\n",
        "reply_percent = []\n",
        "\n",
        "for i in range (len(time)-1):\n",
        "    left = time[i]\n",
        "    right = time[i+1]\n",
        "    \n",
        "    \n",
        "    reply_percent.append(replied_percent(left,right,df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xCGG4npvGFG"
      },
      "outputs": [],
      "source": [
        "print(reply_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I386PXr4T72C"
      },
      "outputs": [],
      "source": [
        "def AinB (A, B):\n",
        "    check =  any(item in A for item in B)\n",
        "    return check\n",
        "\n",
        "def del_bot(name):\n",
        "    tokens = re.split('[^a-zA-Z]', name)\n",
        "    \n",
        "    not_list = ['not', 'Not', 'NOt', 'NOT', 'nOt', 'nOT', 'noT', 'NoT']\n",
        "    \n",
        "    bot_list = ['bot', 'Bot', 'BOt', 'BOT', 'bOt', 'bOT', 'boT', 'BoT']\n",
        "    \n",
        "    if AinB(not_list, tokens) and AinB(bot_list, tokens):\n",
        "        return False\n",
        "\n",
        "    elif AinB(bot_list, tokens):\n",
        "        return True\n",
        "    \n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPmlIfjiT8jt"
      },
      "outputs": [],
      "source": [
        "# create a copy of the dataframe\n",
        "botless_df = df.copy()\n",
        "\n",
        "drop_indices = []\n",
        "\n",
        "for i in range (len(botless_df['author'])):\n",
        "    if del_bot(botless_df.author.iloc[i]):\n",
        "        drop_indices.append(i)\n",
        "drop_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx__P5qea90i"
      },
      "outputs": [],
      "source": [
        "botless_df = botless_df.loc[~botless_df.index.isin(drop_indices)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnWP1Imxe62_"
      },
      "outputs": [],
      "source": [
        "# workout the percentage of deleted commenets within the time range\n",
        "botless_unwanted_percent = []\n",
        "\n",
        "for i in range (len(time)-1):\n",
        "    left = time[i]\n",
        "    right = time[i+1]\n",
        "    \n",
        "    \n",
        "    botless_unwanted_percent.append(deleted_comments(left,right,botless_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le3XNzd9e8k-"
      },
      "outputs": [],
      "source": [
        "botless_unwanted_percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3h47BlMe-hd"
      },
      "outputs": [],
      "source": [
        "# workout the percentage of undeleted commenets got replied and deleted comments got replied within the time range\n",
        "botless_reply_percent = []\n",
        "\n",
        "for i in range (len(time)-1):\n",
        "    left = time[i]\n",
        "    right = time[i+1]\n",
        "    \n",
        "    \n",
        "    botless_reply_percent.append(replied_percent(left,right,botless_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9talnUwfAiK"
      },
      "outputs": [],
      "source": [
        "botless_reply_percent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXOjnBXfN-IE"
      },
      "source": [
        "### **Formal Workflow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "srIvgdX4OE36"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import re\n",
        "import networkx as nx\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Clean the MONTH dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Auxilary functions to remove bot in the raw dataframe\n",
        "\"\"\"\n",
        "def AinB (A, B):\n",
        "    check =  any(item in A for item in B)\n",
        "    return check\n",
        "\n",
        "def del_bot(name):\n",
        "    tokens = re.split('[^a-zA-Z]', name)\n",
        "    \n",
        "    not_list = ['not', 'Not', 'NOt', 'NOT', 'nOt', 'nOT', 'noT', 'NoT']\n",
        "    \n",
        "    bot_list = ['bot', 'Bot', 'BOt', 'BOT', 'bOt', 'bOT', 'boT', 'BoT']\n",
        "    \n",
        "    if AinB(not_list, tokens) and AinB(bot_list, tokens):\n",
        "        return False\n",
        "\n",
        "    elif AinB(bot_list, tokens):\n",
        "        return True\n",
        "    \n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def delBot(name):\n",
        "    tokens = re.split('[^a-zA-Z]', name)\n",
        "    \n",
        "    not_list = ['not', 'Not', 'NOt', 'NOT', 'nOt', 'nOT', 'noT', 'NoT']\n",
        "    \n",
        "    bot_list = ['bot', 'Bot', 'BOt', 'BOT', 'bOt', 'bOT', 'boT', 'BoT']\n",
        "    \n",
        "    if re.search('\\b{[not_list]}\\b', name) and re.search('\\b{[bot_list]}\\b', name):\n",
        "        return False\n",
        "    elif re.search('\\b{[bot_list]}\\b', name):\n",
        "        return True\n",
        "    \n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "name = \"bot\"\n",
        "\n",
        "delBot(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_name_id_dict(dataframe):\n",
        "    \"\"\"\n",
        "    A function that is used to create a name:id dictionary\n",
        "    Args:\n",
        "        dataframe (pandas.dataframe): the dataframe to be exmained\n",
        "\n",
        "    Returns:\n",
        "        name_id: a dictionary having the id as the key, its corresponding name as value\n",
        "    \"\"\"\n",
        "    name_id = {}\n",
        "    for i in range(dataframe.shape[0]):\n",
        "        key = dataframe.id.iloc[i]\n",
        "        value = dataframe.author.iloc[i]\n",
        "\n",
        "        key_value_pair = {key:value}\n",
        "\n",
        "        name_id.update(key_value_pair)\n",
        "    return name_id\n",
        "\n",
        "def clean_by_parent(dataframe, name_id):\n",
        "    \"\"\"\n",
        "    A function that is used to clean any identified BOT from the parent_id in a dataframe\n",
        "    Args:\n",
        "        dataframe (pandas.dataframe): the dataframe to be exmained\n",
        "        name_id (dictionary): a dictionary having the id as the key, its corresponding name as value\n",
        "\n",
        "    Returns:\n",
        "        dataframe: the dataframe that is cleaned by parent_id\n",
        "    \"\"\"\n",
        "    drop_indices = []\n",
        "\n",
        "    for i in range(dataframe.shape[0]):\n",
        "        uid = dataframe.parent_id.iloc[i].split('_')[-1]\n",
        "        # use the dictionary to find the corresponding author name\n",
        "        try:\n",
        "            author_name = name_id[uid]\n",
        "        except:\n",
        "            # this would happen if the poster created the post in posterior months\n",
        "            continue\n",
        "        if del_bot(author_name):\n",
        "            drop_indices.append(i)\n",
        "    \n",
        "    dataframe = dataframe.drop(drop_indices)\n",
        "    \n",
        "    return dataframe.reset_index(drop=True)\n",
        "\n",
        "def clean_author(dataframe):\n",
        "    \"\"\"\n",
        "    A function that is used to clean any identified BOT from the author in a dataframe\n",
        "    Args:\n",
        "        dataframe (pandas.dataframe): the dataframe to be exmained\n",
        "    Returns:\n",
        "        dataframe: the dataframe that is cleaned by author name\n",
        "    \"\"\"\n",
        "    drop_indices = []\n",
        "\n",
        "    for i in range(dataframe.shape[0]):\n",
        "        author_name = dataframe.author.iloc[i]\n",
        "\n",
        "        if del_bot(author_name):\n",
        "            drop_indices.append(i)\n",
        "    \n",
        "    dataframe = dataframe.drop(drop_indices)\n",
        "\n",
        "    return dataframe.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A compact function that does the cleaning for a raw dataframe\n",
        "\"\"\"\n",
        "def clean_df(dataframe):\n",
        "    name_id = create_name_id_dict(dataframe)\n",
        "\n",
        "    dataframe = clean_by_parent(dataframe, name_id)\n",
        "\n",
        "    dataframe = clean_author(dataframe)\n",
        "\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Create a list of DAY dataframe from a MONTH dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def workout_time(year, month):\n",
        "  \"\"\"\n",
        "  A function that works out the timestamps of each day in a month\n",
        "  Args:\n",
        "      year (int): the year\n",
        "      month (int): the month\n",
        "\n",
        "  Returns:\n",
        "      day_list: the timestamp of each day in a month\n",
        "  \"\"\"\n",
        "  max_day = 31\n",
        "  month_list = [4, 6, 9, 11]\n",
        "\n",
        "  if year == 2020 and month == 2:\n",
        "    max_day = 29\n",
        "  elif month == 2:\n",
        "    max_day = 28\n",
        "  elif month in month_list:\n",
        "    max_day = 30\n",
        "  \n",
        "  start_date = datetime(year, month, 1, 0) # always start at the first day on each month\n",
        "\n",
        "  day_list = []\n",
        "\n",
        "  for i in range(max_day):\n",
        "    count_time = start_date + relativedelta(days=i)\n",
        "    count_time = count_time.timestamp()\n",
        "    day_list.append(count_time)\n",
        "  \n",
        "  return day_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk3VVx6oOxGl"
      },
      "outputs": [],
      "source": [
        "# A compact function that works out a list of DAY dataframe from a MONTH dataframe\n",
        "def create_subframe(year, month, dataframe):\n",
        "  \"\"\"\n",
        "  A funtion that creates sub_dataframes from the parent dataframe based on number of days\n",
        "  Args:\n",
        "      year (int): the year\n",
        "      month (int): the month\n",
        "      dataframe(pandas.dataframe): the MONTH dataframe we are examine\n",
        "\n",
        "  Returns:\n",
        "      df_list: a list of DAY dataframe\n",
        "  \"\"\"\n",
        "  df_list = []\n",
        "\n",
        "  day_list = workout_time(year, month)\n",
        "\n",
        "  for i in range(len(day_list)-1):\n",
        "    left = day_list[i]\n",
        "    right = day_list[i+1]\n",
        "\n",
        "    day_df = dataframe[(dataframe.created_utc >= left) & (dataframe.created_utc < right)]\n",
        "    \n",
        "    df_list.append(day_df)\n",
        "  \n",
        "  return df_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------\n",
        "<h3>Create the OUTPUT dataframe for each DAY dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Auxilary functions to create an OUTPUT dataframe\n",
        "\"\"\"\n",
        "def create_output_df(daydf):\n",
        "    \"\"\"\n",
        "    A function that creates an output dataframe for us to create the edge list later\n",
        "    Args:\n",
        "        daydf (pandas.dataframe): a DAY dataframe to be examined\n",
        "\n",
        "    Returns:\n",
        "        main_authors: a list of authors the gives the main post\n",
        "        output_df: the output dataframe we want\n",
        "    \"\"\"\n",
        "    tqdm.pandas()\n",
        "    level_1_comments = daydf[daydf['parent_id'] == daydf['link_id']]\n",
        "    main_authors = level_1_comments['author'].tolist()\n",
        "    sub_comments = daydf[~daydf.index.isin(level_1_comments.index)] #setting the sub comments being those with index that are not in level_1_comments\n",
        "\n",
        "    output_df  = sub_comments.groupby('parent_id')['author'].apply(list).reset_index(name='author')\n",
        "    output_df['sub_comments'] = output_df['author'].progress_apply(lambda x:len(x))\n",
        "    output_df.sort_values('sub_comments', inplace=True, ascending=False)\n",
        "\n",
        "    return main_authors, output_df\n",
        "\n",
        "def is_main(outputdf, daydf):\n",
        "    \"\"\"\n",
        "    A function that check if a post from the OUTPUT dataframe is recognised as a MAIN post\n",
        "    Args:\n",
        "        outputdf (): the OUTPUT dataframe\n",
        "        daydf (): the DAY dataframe\n",
        "\n",
        "    Returns:\n",
        "        outputdf: with two more columns. One is 'main_post': where 1=main post; 0 otherwise\n",
        "                                         The other is 'parent_author', gives the author name of the corresponding parent id\n",
        "    \"\"\"\n",
        "    main_list = []\n",
        "    parent_author_list = []\n",
        "\n",
        "    for i in range(outputdf.shape[0]):\n",
        "        uid = outputdf.parent_id[i].split('_')[-1]\n",
        "        ind = daydf[daydf['id'] == uid].index.to_list()\n",
        "        # print(ind)\n",
        "        if ind != []:\n",
        "            parent_author_list.append(daydf.loc[ind[0], 'author'])\n",
        "            if daydf.loc[ind[0], 'parent_id'] == daydf.loc[ind[0], 'link_id']:\n",
        "                main_list.append(1)\n",
        "            else:\n",
        "                main_list.append(0)\n",
        "        else:\n",
        "            main_list.append(np.nan)\n",
        "            parent_author_list.append(np.nan)\n",
        "            \n",
        "    outputdf['main_post'] =  main_list\n",
        "    outputdf['parent_author'] = parent_author_list\n",
        "    \n",
        "    return outputdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6Ei8C09PBax"
      },
      "outputs": [],
      "source": [
        "# A compact function that works out the OUTPUT dataframe\n",
        "def output_dataframe(daydf):\n",
        "    \"\"\"\n",
        "    A function that works out the OUTPUT dataframe from a DAY dataframe.\n",
        "    5 columns: parent_id, author, sub_comments, main_post and parent_author\n",
        "    Args:\n",
        "        daydf (): the DAY dataframe\n",
        "\n",
        "    Returns:\n",
        "        outputdf: the OUTPUT dataframe that is being reset index\n",
        "    \"\"\"\n",
        "    main_authors, outputdf = create_output_df(daydf)\n",
        "\n",
        "    outputdf = is_main(outputdf, daydf)\n",
        "\n",
        "    return main_authors, outputdf.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-------------------------------\n",
        "<h3>Create the adjacency matrix for a OUTPUT dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Auxilary functions to create the adj matrix\n",
        "\"\"\"\n",
        "def get_all_authors(name_id_day, output_df):\n",
        "    \"\"\"\n",
        "    A function to get all author names who created the post on this DAY from a OUTPUT dataframe \n",
        "    Args:\n",
        "        name_id_day (dictionary): a dictionary having the id as the key, its corresponding name as value from a DAY dataframe\n",
        "        output_df (pandas.dataframe): the output dataframe to be examined\n",
        "\n",
        "    Returns:\n",
        "        author_list: a list of author names\n",
        "    \"\"\"\n",
        "\n",
        "    author_list = set()\n",
        "    for i in range(output_df.shape[0]):\n",
        "        uid = output_df.parent_id.iloc[i].split('_')[-1]\n",
        "\n",
        "        try:\n",
        "            author_list.add(name_id_day[uid])\n",
        "        except:\n",
        "            # this would happend if author created the post on a posterior day\n",
        "            continue\n",
        "\n",
        "        author_list.update(tuple(output_df.author.iloc[i]))\n",
        "        \n",
        "    return author_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A compact function that returns the adj matrix\n",
        "def generate_adj_matrix(outputdf, name_id_day): \n",
        "    \"\"\"\n",
        "    A function to generate the adjacency matrix\n",
        "    Args:\n",
        "        outputdf (): the OUTPUT dataframe of the corresponding DAY dataframe\n",
        "        name_id_day (dictionary): a dictionary having the id as the key, its corresponding name as value from a DAY dataframe\n",
        "\n",
        "    Returns:\n",
        "        adj_matrix: the matrix\n",
        "    \"\"\"\n",
        "    # create the dict from the DAY dataframe\n",
        "    author_list = get_all_authors(name_id_day, outputdf)\n",
        "\n",
        "    adj_matrix = pd.DataFrame(0, index = author_list, columns = author_list)\n",
        "\n",
        "    for i in range(outputdf.shape[0]):\n",
        "        uid = outputdf.parent_id.iloc[i].split('_')[-1]\n",
        "        \n",
        "        try:\n",
        "            parent_name = name_id_day[uid]\n",
        "        except:\n",
        "            continue\n",
        "        author_list = outputdf.author.iloc[i]\n",
        "\n",
        "        for author in author_list:\n",
        "            adj_matrix.loc[author, parent_name] += 1\n",
        "    \n",
        "    return adj_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------\n",
        "<h3>Create an edge list from a DAY dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGJOxLYsJOmF"
      },
      "outputs": [],
      "source": [
        "def get_edge_list(outputdf, name_id_day, main_authors): \n",
        "    \"\"\"\n",
        "    A function to generate edge list dataframe\n",
        "    Args:\n",
        "        outputdf (): the OUTPUT dataframe\n",
        "        name_id_day (dictionary): a dictionary having the id as the key, its corresponding name as value from a DAY dataframe\n",
        "        main_authors (list): a list of authors the gives the main post\n",
        "\n",
        "    Returns:\n",
        "        edge_list: a dataframe of edge list with 3 colums, each are Source, Target and is_main\n",
        "    \"\"\"\n",
        "    source_list = []\n",
        "    target_list = []\n",
        "    is_main_list = []\n",
        "    for i in range(outputdf.shape[0]):\n",
        "        uid = outputdf.parent_id.iloc[i].split('_')[-1]        \n",
        "        try:\n",
        "            parent_name = name_id_day[uid]\n",
        "            main_post = 1 if parent_name in main_authors else  0 \n",
        "            author_list = outputdf.author.iloc[i]\n",
        "            for author in author_list:\n",
        "                source_list.append(author)\n",
        "                target_list.append(parent_name)\n",
        "                is_main_list.append(main_post)\n",
        "        except:\n",
        "            continue\n",
        "    edge_list = pd.DataFrame(list(zip(source_list, target_list, is_main_list)), columns = ['Source', 'Target', 'is_main'])\n",
        "\n",
        "    return edge_list\n",
        "\n",
        "def get_singleton(outputdf, daydf):\n",
        "    \"\"\"\n",
        "    A function that works out a list of authors recognised as singleton nodes in the edge list\n",
        "    Args:\n",
        "        outputdf (): the OUTPUT dataframe \n",
        "        daydf (): the DAY dataframe \n",
        "\n",
        "    Returns:\n",
        "        singleton: a list of author name that are singleton\n",
        "    \"\"\"\n",
        "    \n",
        "    complete_authors = daydf['author'].to_list()\n",
        "    non_single = []\n",
        "    for i in range(outputdf.shape[0]):\n",
        "        if outputdf.parent_author.iloc[i] == np.nan:\n",
        "            continue\n",
        "        else:\n",
        "            non_single.append(outputdf.parent_author.iloc[i])\n",
        "            non_single += outputdf.author.iloc[i]\n",
        "    non_single = list(dict.fromkeys(non_single))\n",
        "    complete_authors = list(dict.fromkeys(complete_authors))\n",
        "    singleton = set(complete_authors) - set(non_single)\n",
        "    \n",
        "    return singleton\n",
        "\n",
        "def singleton_edge(edge_list, singleton):\n",
        "    \"\"\"\n",
        "    A function adds singleton nodes to an existing edge list\n",
        "    Args:\n",
        "        edge_list (): the edge list\n",
        "        singleton (list):  a list of author name that are singleton\n",
        "\n",
        "    Returns:\n",
        "        edge_list: the updated edge list where SOURCE=TARGET=author from singleton and WEIGHT = 1\n",
        "    \"\"\"\n",
        "    \n",
        "    for author in singleton:\n",
        "        row = [author, author, 1]\n",
        "        edge_list.loc[len(edge_list.index)] = row\n",
        "    \n",
        "    return edge_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A compact function that returns two different edge list from an OUTPUT dataframe\n",
        "def e_lists(outputdf, daydf, main_authors, name_id_day):\n",
        "    standard_e = get_edge_list(outputdf, name_id_day, main_authors)\n",
        "\n",
        "    # to return the updated edge list\n",
        "    single = get_singleton(outputdf, daydf)\n",
        "\n",
        "    updated_e = standard_e[['Source', 'Target']]\n",
        "    updated_e = updated_e.groupby(updated_e.columns.tolist()).size().reset_index().rename(columns={0:'weights'})\n",
        "\n",
        "    updated_e = singleton_edge(updated_e, single)\n",
        "\n",
        "    return standard_e, updated_e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------------\n",
        "<h2>To walk throught a month</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a CHEAP month walker by now\n",
        "def month_walker(raw_df, year, month):\n",
        "    \"\"\"\n",
        "    A function returns a list of DAY information from a DAY dataframe inside a list of DAY-frames\n",
        "    The information has: 1. OUTPUT dataframe, 2. Adjacency matrix and 3. Two types of edge lists\n",
        "    Each information is of type tuple\n",
        "    Args:\n",
        "        raw_df (): dataframe not cleaned\n",
        "        year (int): the year\n",
        "        month (int): the month\n",
        "\n",
        "    Returns:\n",
        "        month_info: a list of information\n",
        "    \"\"\"\n",
        "    # clean dataframe\n",
        "    raw_df = raw_df.reset_index(drop=True)\n",
        "    cleaned_df = clean_df(raw_df)\n",
        "    # create a list of DAY dataframes\n",
        "    df_list = create_subframe(year, month, cleaned_df)\n",
        "\n",
        "    month_info = []\n",
        "\n",
        "    for daydf in df_list:\n",
        "        # this gives the name id dictionary for TODAY\n",
        "        name_id_day = create_name_id_dict(daydf)\n",
        "        # this gives you the OUTPUT dataframe\n",
        "        main_authors, outputdf = output_dataframe(daydf)\n",
        "        # this gives the adj matrix\n",
        "        adj_mat = generate_adj_matrix(outputdf, name_id_day)\n",
        "        # this gives the two edge lists\n",
        "        standard_e, updated_e = e_lists(outputdf, daydf, main_authors, name_id_day)\n",
        "\n",
        "        day_info = (outputdf, adj_mat, standard_e, updated_e)\n",
        "\n",
        "        month_info.append(day_info)\n",
        "    \n",
        "    return month_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fp1 = \"/home/vw/Desktop/UST_summer/01-20/01-20_comments.csv\"\n",
        "df_01 = pd.read_csv(fp1, low_memory = False, encoding = 'utf-8')  #get the Jan-2020 spreadsheet\n",
        "df_01 = df_01[df_01.author != '[deleted]'] #remove authors that are deleted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "month, year = 1, 2020\n",
        "raw_df = df_01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jan_2020_info = month_walker(raw_df,year,month)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "post_authors = pd.read_csv('/Users/victor/Desktop/CS/UST_summer/posts_authors.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('/Users/victor/Desktop/CS/UST_summer/main_authors.pickle', 'rb') as handle:\n",
        "    main_authors = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('/Users/victor/Desktop/CS/UST_summer/name_id.pickle', 'rb') as handle:\n",
        "    name_id = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "sliced_post = post_authors[:100000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_main(dataframe, name_id_dictionary, authorlist):\n",
        "    main_list = []\n",
        "    parent_author_list = []\n",
        "    dataframe = dataframe.reset_index(drop = True)\n",
        "    for i in range(dataframe.shape[0]):\n",
        "        uid = dataframe.parent_id[i].split('_')[-1]\n",
        "        \n",
        "        try:\n",
        "            parent_author = name_id_dictionary[uid]\n",
        "            parent_author_list.append(parent_author)\n",
        "            \n",
        "            if parent_author in authorlist:\n",
        "                main_list.append(1)\n",
        "            else:\n",
        "                main_list.append(0)\n",
        "        except:\n",
        "            main_list.append(np.nan)\n",
        "            parent_author_list.append(np.nan)\n",
        "            \n",
        "        try:\n",
        "            if i % 100000 == 0:\n",
        "                main_filename = 'main_list'+str(i)+'.pickle'\n",
        "                post_filename = 'post_list'+str(i)+'.pickle'\n",
        "                with open(main_filename, 'wb') as handle:\n",
        "                    pickle.dump(main_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                with open(post_filename, 'wb') as handle:\n",
        "                    pickle.dump(parent_author_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        except:\n",
        "            print('kernel broken at i = ' + str(i))\n",
        "    dataframe['main_post'] =  main_list\n",
        "    dataframe['parent_author'] = parent_author_list\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n",
            "30000\n",
            "30100\n",
            "30200\n",
            "30300\n",
            "30400\n",
            "30500\n",
            "30600\n",
            "30700\n",
            "30800\n",
            "30900\n",
            "31000\n",
            "31100\n",
            "31200\n",
            "31300\n",
            "31400\n",
            "31500\n",
            "31600\n",
            "31700\n",
            "31800\n",
            "31900\n",
            "32000\n",
            "32100\n",
            "32200\n",
            "32300\n",
            "32400\n",
            "32500\n",
            "32600\n",
            "32700\n",
            "32800\n",
            "32900\n",
            "33000\n",
            "33100\n",
            "33200\n",
            "33300\n",
            "33400\n",
            "33500\n",
            "33600\n",
            "33700\n",
            "33800\n",
            "33900\n",
            "34000\n",
            "34100\n",
            "34200\n",
            "34300\n",
            "34400\n",
            "34500\n",
            "34600\n",
            "34700\n",
            "34800\n",
            "34900\n",
            "35000\n",
            "35100\n",
            "35200\n",
            "35300\n",
            "35400\n",
            "35500\n",
            "35600\n",
            "35700\n",
            "35800\n",
            "35900\n",
            "36000\n",
            "36100\n",
            "36200\n",
            "36300\n",
            "36400\n",
            "36500\n",
            "36600\n",
            "36700\n",
            "36800\n",
            "36900\n",
            "37000\n",
            "37100\n",
            "37200\n",
            "37300\n",
            "37400\n",
            "37500\n",
            "37600\n",
            "37700\n",
            "37800\n",
            "37900\n",
            "38000\n",
            "38100\n",
            "38200\n",
            "38300\n",
            "38400\n",
            "38500\n",
            "38600\n",
            "38700\n",
            "38800\n",
            "38900\n",
            "39000\n",
            "39100\n",
            "39200\n",
            "39300\n",
            "39400\n",
            "39500\n",
            "39600\n",
            "39700\n",
            "39800\n",
            "39900\n",
            "40000\n",
            "40100\n",
            "40200\n",
            "40300\n",
            "40400\n",
            "40500\n"
          ]
        }
      ],
      "source": [
        "new_post = is_main(sliced_post, name_id, main_authors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLFVJqwNLUVH"
      },
      "outputs": [],
      "source": [
        "# def month_walker(month_df, path, mm_yy, save=True):\n",
        "#     if save:\n",
        "#         folder_path = os.path.join(path, mm_yy)\n",
        "#         os.makedirs(folder_path) \n",
        "#     for day_df in month_df:\n",
        "#         i = 1\n",
        "#         # first we create a pc_df and a dictionary to store the name\n",
        "#         main_authors, posts_authors = create_parent_children_df(day_df)\n",
        "#         name_id = create_name_id_dict(day_df)\n",
        "#         # then we clean pc_df by parent_id and its author column\n",
        "#         posts_authors = posts_authors.drop(clean_by_parent(posts_authors, name_id))\n",
        "#         posts_authors = clean_author(posts_authors)\n",
        "#         # finally create the adj matrix\n",
        "#         author_list = get_all_authors(name_id, posts_authors)\n",
        "#         source_list = []\n",
        "#         target_list = []\n",
        "#         is_main_list = []\n",
        "#         for i in range(dataframe.shape[0]):\n",
        "#           uid = dataframe.parent_id.iloc[i].split('_')[-1]        \n",
        "#           try:\n",
        "#             parent_name = name_id_dict[uid]\n",
        "#             main_post = 1 if parent_name in main_authors else  0 \n",
        "#             author_list = dataframe.author.iloc[i]\n",
        "#             for author in author_list:\n",
        "#               source_list.append(author)\n",
        "#               target_list.append(parent_name)\n",
        "#               is_main_list.append(main_post)\n",
        "#           except:\n",
        "#             continue\n",
        "#         edge_list = pd.DataFrame(list(zip(source_list, target_list, is_main_list)), columns = ['Source', 'Target', 'is_main'])\n",
        "#         # store the matrix as pickle\n",
        "#         save_path = os.path.join(folder_path, str(i))\n",
        "#         edge_list.to_pickle(save_path)\n",
        "#         i += 1\n",
        "    \n",
        "#     return matrices"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "reddits.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "864791b727c67594ba35b846e2fef3ab0cf2138c660c288cb4b982d81a4cb57f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
