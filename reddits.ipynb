{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMakfN9bn0CB"
      },
      "source": [
        "<h1>UST Reddit Projecy</h1></r>\n",
        "<h2>Pipeline</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmIuSrbLeJLQ",
        "outputId": "605d7ac7-293f-4e30-86aa-a059c3745af2"
      },
      "outputs": [],
      "source": [
        "# gives accessibility to my goodle drive to fetch the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7xATcS5n0CD"
      },
      "source": [
        "<h2>Generate dataset</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1dZSTNTn0CD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import re\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuCLqUN4n0CE"
      },
      "source": [
        "<h3>Read the raw data WITHOUT CLEANING</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GbQ4ljtn0CE",
        "outputId": "70950837-9915-41c4-e356-0ef314e3d881"
      },
      "outputs": [],
      "source": [
        "fp = '/Users/victor/Desktop/CS/UST_summer/all_posts_clean_1.csv' # the directory of the raw data file, pipeliend from reddits\n",
        "\n",
        "s_columns = ['id', 'author', 'created_utc', 'num_comments', 'clean_title', 'clean_selftext', 'num_title_emojis', 'num_text_emojis', 'title_length', 'text_length', 'score', 'num_replied_comments', 'num_replies_by_new', 'list_new_repliers', 'num_replies_by_old', 'list_old_repliers']\n",
        "\n",
        "df = pd.read_csv(fp, low_memory=False, encoding='utf-8')[s_columns]\n",
        "\n",
        "df.head() # test if it is read correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNXN3S32n0CE"
      },
      "outputs": [],
      "source": [
        "# df = df[df.created_utc < 1590000000.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDPlszy6n0CF",
        "outputId": "072d52d1-4985-4765-b016-1b8238463724"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "df.groupby('author').cumcount()\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "df[\"num_post\"] = df.groupby('author').cumcount().to_numpy()\n",
        "codes, uniques = pd.factorize(df[\"author\"])\n",
        "df[\"author_codes\"] = codes\n",
        "\n",
        "# df.to_csv('wsb_with_authorcode_num_post.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGy-bVvFn0CF"
      },
      "source": [
        "<h2>for statistical tests ONLY</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlVRJONQn0CF"
      },
      "outputs": [],
      "source": [
        "indices = df.index[(df.created_utc >= 1609459200.0) & (df.created_utc <= 1612137599.0)].tolist()\n",
        "\n",
        "# zeros = np.zeros(df.shape[0])\n",
        "# for i in indices : zeros[i] = 1\n",
        "\n",
        "# df['is_2021Jan'] = zeros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzIWsZ-On0CG",
        "outputId": "ead05f8f-9fcd-444b-e13e-1bbfd9f1862d"
      },
      "outputs": [],
      "source": [
        "number_of_replies = list()\n",
        "\n",
        "index = df.index[(df['clean_selftext'] == 'deleted') | (df['clean_selftext'] == 'removed')].tolist()\n",
        "for i in index:\n",
        "    number_of_replies.append(df['num_comments'].iloc[i])\n",
        "    \n",
        "# len(number_of_replies)\n",
        "\n",
        "nonzero_replies = np.count_nonzero(np.array(number_of_replies))\n",
        "nonzero_replies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF_KjIx9n0CG",
        "outputId": "67d9da3b-dd76-4bb6-cfe7-d0976caada9d"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTzczjq0n0CG"
      },
      "outputs": [],
      "source": [
        "# count the number of deleted and removed comments in each month from 2020-01-01 to 2021-05-01\n",
        "date = datetime(2020,1,1,0)\n",
        "time = []\n",
        "\n",
        "for i in range(17):\n",
        "    count_time = date + relativedelta(months=i)\n",
        "    count_time = count_time.timestamp()\n",
        "    time.append(count_time)\n",
        "\n",
        "time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXA_8IDjn0CH"
      },
      "outputs": [],
      "source": [
        "def deleted_comments(left_bound, right_bound, dataframe):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        left_bound (_type_): _description_\n",
        "        right_bound (_type_): _description_\n",
        "        dataframe (_type_): _description_\n",
        "    \"\"\"\n",
        "    indices = dataframe.index[(dataframe.created_utc >= left_bound) & (dataframe.created_utc <= right_bound)].tolist() # workout the total number of comments\n",
        "    \n",
        "    # print(indices)\n",
        "    if len(indices) == 0:\n",
        "        return 0\n",
        "    \n",
        "    unwanted_comments = 0\n",
        "    for i in indices:\n",
        "        if (dataframe['clean_selftext'].iloc[i] == 'deleted') | (dataframe['clean_selftext'].iloc[i] == 'removed'):\n",
        "            unwanted_comments += 1\n",
        "    \n",
        "    return (unwanted_comments/len(indices))*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym8wftLhn0CH"
      },
      "outputs": [],
      "source": [
        "# workout the percentage of deleted commenets within the time range\n",
        "unwanted_percent = []\n",
        "\n",
        "for i in range (len(time)-1):\n",
        "    left = time[i]\n",
        "    right = time[i+1]\n",
        "    \n",
        "    \n",
        "    unwanted_percent.append(deleted_comments(left,right,df))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WisDbEOGn0CH"
      },
      "outputs": [],
      "source": [
        "def replied_percent(left_bound, right_bound, dataframe):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        left_bound (_type_): _description_\n",
        "        right_bound (_type_): _description_\n",
        "        dataframe (_type_): _description_\n",
        "    \"\"\"\n",
        "    indices = dataframe.index[(dataframe.created_utc >= left_bound) & (dataframe.created_utc <= right_bound)].tolist()\n",
        "    \n",
        "    negative_index = []\n",
        "    negative_positive = 0\n",
        "    \n",
        "    for i in indices:\n",
        "        if (dataframe['clean_selftext'].iloc[i] == 'deleted') | (dataframe['clean_selftext'].iloc[i] == 'removed'):\n",
        "            negative_index.append(i)\n",
        "    \n",
        "    for i in negative_index:\n",
        "        if dataframe.num_comments.iloc[i] > 0:\n",
        "            negative_positive += 1\n",
        "    \n",
        "    negative_positive_rate = negative_positive / len(negative_index)\n",
        "    \n",
        "    positive_index = list(set(indices) - set(negative_index))\n",
        "    positive_positive = 0\n",
        "    \n",
        "    for i in positive_index:\n",
        "        if dataframe.num_comments.iloc[i] > 0:\n",
        "            positive_positive += 1\n",
        "    \n",
        "    positive_positive_rate = positive_positive / len(positive_index)\n",
        "    \n",
        "    return (positive_positive_rate, negative_positive_rate)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y_NdrcGvDhz"
      },
      "outputs": [],
      "source": [
        "# workout the percentage of undeleted commenets got replied and deleted comments got replied within the time range\n",
        "reply_percent = []\n",
        "\n",
        "for i in range (len(time)-1):\n",
        "    left = time[i]\n",
        "    right = time[i+1]\n",
        "    \n",
        "    \n",
        "    reply_percent.append(replied_percent(left,right,df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xCGG4npvGFG"
      },
      "outputs": [],
      "source": [
        "print(reply_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I386PXr4T72C"
      },
      "outputs": [],
      "source": [
        "def AinB (A, B):\n",
        "    check =  any(item in A for item in B)\n",
        "    return check\n",
        "\n",
        "def del_bot(name):\n",
        "    tokens = re.split('[^a-zA-Z]', name)\n",
        "    \n",
        "    not_list = ['not', 'Not', 'NOt', 'NOT', 'nOt', 'nOT', 'noT', 'NoT']\n",
        "    \n",
        "    bot_list = ['bot', 'Bot', 'BOt', 'BOT', 'bOt', 'bOT', 'boT', 'BoT']\n",
        "    \n",
        "    if AinB(not_list, tokens) and AinB(bot_list, tokens):\n",
        "        return False\n",
        "\n",
        "    elif AinB(bot_list, tokens):\n",
        "        return True\n",
        "    \n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPmlIfjiT8jt"
      },
      "outputs": [],
      "source": [
        "# create a copy of the dataframe\n",
        "botless_df = df.copy()\n",
        "\n",
        "drop_indices = []\n",
        "\n",
        "for i in range (len(botless_df['author'])):\n",
        "    if del_bot(botless_df.author.iloc[i]):\n",
        "        drop_indices.append(i)\n",
        "drop_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx__P5qea90i"
      },
      "outputs": [],
      "source": [
        "botless_df = botless_df.loc[~botless_df.index.isin(drop_indices)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnWP1Imxe62_"
      },
      "outputs": [],
      "source": [
        "# workout the percentage of deleted commenets within the time range\n",
        "botless_unwanted_percent = []\n",
        "\n",
        "for i in range (len(time)-1):\n",
        "    left = time[i]\n",
        "    right = time[i+1]\n",
        "    \n",
        "    \n",
        "    botless_unwanted_percent.append(deleted_comments(left,right,botless_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le3XNzd9e8k-"
      },
      "outputs": [],
      "source": [
        "botless_unwanted_percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3h47BlMe-hd"
      },
      "outputs": [],
      "source": [
        "# workout the percentage of undeleted commenets got replied and deleted comments got replied within the time range\n",
        "botless_reply_percent = []\n",
        "\n",
        "for i in range (len(time)-1):\n",
        "    left = time[i]\n",
        "    right = time[i+1]\n",
        "    \n",
        "    \n",
        "    botless_reply_percent.append(replied_percent(left,right,botless_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9talnUwfAiK"
      },
      "outputs": [],
      "source": [
        "botless_reply_percent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXOjnBXfN-IE"
      },
      "source": [
        "### **Formal Workflow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srIvgdX4OE36"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import re\n",
        "import networkx as nx\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Clean the MONTH dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Auxilary functions to remove bot in the raw dataframe\n",
        "\"\"\"\n",
        "def AinB (A, B):\n",
        "    check =  any(item in A for item in B)\n",
        "    return check\n",
        "\n",
        "def del_bot(name):\n",
        "    tokens = re.split('[^a-zA-Z]', name)\n",
        "    \n",
        "    not_list = ['not', 'Not', 'NOt', 'NOT', 'nOt', 'nOT', 'noT', 'NoT']\n",
        "    \n",
        "    bot_list = ['bot', 'Bot', 'BOt', 'BOT', 'bOt', 'bOT', 'boT', 'BoT']\n",
        "    \n",
        "    if AinB(not_list, tokens) and AinB(bot_list, tokens):\n",
        "        return False\n",
        "\n",
        "    elif AinB(bot_list, tokens):\n",
        "        return True\n",
        "    \n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_name_id_dict(dataframe):\n",
        "    \"\"\"\n",
        "    A function that is used to create a name:id dictionary\n",
        "    Args:\n",
        "        dataframe (pandas.dataframe): the dataframe to be exmained\n",
        "\n",
        "    Returns:\n",
        "        name_id: a dictionary having the id as the key, its corresponding name as value\n",
        "    \"\"\"\n",
        "    name_id = {}\n",
        "    for i in range(dataframe.shape[0]):\n",
        "        key = dataframe.id.iloc[i]\n",
        "        value = dataframe.author.iloc[i]\n",
        "\n",
        "        key_value_pair = {key:value}\n",
        "\n",
        "        name_id.update(key_value_pair)\n",
        "    return name_id\n",
        "\n",
        "def clean_by_parent(dataframe, name_id):\n",
        "    \"\"\"\n",
        "    A function that is used to clean any identified BOT from the parent_id in a dataframe\n",
        "    Args:\n",
        "        dataframe (pandas.dataframe): the dataframe to be exmained\n",
        "        name_id (dictionary): a dictionary having the id as the key, its corresponding name as value\n",
        "\n",
        "    Returns:\n",
        "        dataframe: the dataframe that is cleaned by parent_id\n",
        "    \"\"\"\n",
        "    drop_indices = []\n",
        "\n",
        "    for i in range(dataframe.shape[0]):\n",
        "        uid = dataframe.parent_id.iloc[i].split('_')[-1]\n",
        "        # use the dictionary to find the corresponding author name\n",
        "        try:\n",
        "            author_name = name_id[uid]\n",
        "        except:\n",
        "            # this would happen if the poster created the post in posterior months\n",
        "            continue\n",
        "        if del_bot(author_name):\n",
        "            drop_indices.append(i)\n",
        "    \n",
        "    dataframe = dataframe.drop(drop_indices)\n",
        "    \n",
        "    return dataframe.reset_index(drop=True)\n",
        "\n",
        "def clean_author(dataframe):\n",
        "    \"\"\"\n",
        "    A function that is used to clean any identified BOT from the author in a dataframe\n",
        "    Args:\n",
        "        dataframe (pandas.dataframe): the dataframe to be exmained\n",
        "    Returns:\n",
        "        dataframe: the dataframe that is cleaned by author name\n",
        "    \"\"\"\n",
        "    drop_indices = []\n",
        "\n",
        "    for i in range(dataframe.shape[0]):\n",
        "        author_name = dataframe.author.iloc[i]\n",
        "\n",
        "        if del_bot(author_name):\n",
        "            drop_indices.append(i)\n",
        "    \n",
        "    dataframe = dataframe.drop(drop_indices)\n",
        "\n",
        "    return dataframe.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A compact function that does the cleaning for a raw dataframe\n",
        "\"\"\"\n",
        "def clean_df(dataframe):\n",
        "    name_id = create_name_id_dict(dataframe)\n",
        "\n",
        "    dataframe = clean_by_parent(dataframe, name_id)\n",
        "\n",
        "    dataframe = clean_author(dataframe)\n",
        "\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Create a list of DAY dataframe from a MONTH dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def workout_time(year, month):\n",
        "  \"\"\"\n",
        "  A function that works out the timestamps of each day in a month\n",
        "  Args:\n",
        "      year (int): the year\n",
        "      month (int): the month\n",
        "\n",
        "  Returns:\n",
        "      day_list: the timestamp of each day in a month\n",
        "  \"\"\"\n",
        "  max_day = 31\n",
        "  month_list = [4, 6, 9, 11]\n",
        "\n",
        "  if year == 2020 and month == 2:\n",
        "    max_day = 29\n",
        "  elif month == 2:\n",
        "    max_day = 28\n",
        "  elif month in month_list:\n",
        "    max_day = 30\n",
        "  \n",
        "  start_date = datetime(year, month, 1, 0) # always start at the first day on each month\n",
        "\n",
        "  day_list = []\n",
        "\n",
        "  for i in range(max_day):\n",
        "    count_time = start_date + relativedelta(days=i)\n",
        "    count_time = count_time.timestamp()\n",
        "    day_list.append(count_time)\n",
        "  \n",
        "  return day_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk3VVx6oOxGl"
      },
      "outputs": [],
      "source": [
        "# A compact function that works out a list of DAY dataframe from a MONTH dataframe\n",
        "def create_subframe(year, month, dataframe):\n",
        "  \"\"\"\n",
        "  A funtion that creates sub_dataframes from the parent dataframe based on number of days\n",
        "  Args:\n",
        "      year (int): the year\n",
        "      month (int): the month\n",
        "      dataframe(pandas.dataframe): the MONTH dataframe we are examine\n",
        "\n",
        "  Returns:\n",
        "      df_list: a list of DAY dataframe\n",
        "  \"\"\"\n",
        "  df_list = []\n",
        "\n",
        "  day_list = workout_time(year, month)\n",
        "\n",
        "  for i in range(len(day_list)-1):\n",
        "    left = day_list[i]\n",
        "    right = day_list[i+1]\n",
        "\n",
        "    day_df = dataframe[(dataframe.created_utc >= left) & (dataframe.created_utc < right)]\n",
        "    \n",
        "    df_list.append(day_df)\n",
        "  \n",
        "  return df_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------\n",
        "<h3>Create the OUTPUT dataframe for each DAY dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Auxilary functions to create an OUTPUT dataframe\n",
        "\"\"\"\n",
        "def create_output_df(daydf):\n",
        "    \"\"\"\n",
        "    A function that creates an output dataframe for us to create the edge list later\n",
        "    Args:\n",
        "        daydf (pandas.dataframe): a DAY dataframe to be examined\n",
        "\n",
        "    Returns:\n",
        "        main_authors: a list of authors the gives the main post\n",
        "        output_df: the output dataframe we want\n",
        "    \"\"\"\n",
        "    tqdm.pandas()\n",
        "    level_1_comments = daydf[daydf['parent_id'] == daydf['link_id']]\n",
        "    main_authors = level_1_comments['author'].tolist()\n",
        "    sub_comments = daydf[~daydf.index.isin(level_1_comments.index)] #setting the sub comments being those with index that are not in level_1_comments\n",
        "\n",
        "    output_df  = sub_comments.groupby('parent_id')['author'].apply(list).reset_index(name='author')\n",
        "    output_df['sub_comments'] = output_df['author'].progress_apply(lambda x:len(x))\n",
        "    output_df.sort_values('sub_comments', inplace=True, ascending=False)\n",
        "\n",
        "    return main_authors, output_df\n",
        "\n",
        "def is_main(outputdf, daydf):\n",
        "    \"\"\"\n",
        "    A function that check if a post from the OUTPUT dataframe is recognised as a MAIN post\n",
        "    Args:\n",
        "        outputdf (): the OUTPUT dataframe\n",
        "        daydf (): the DAY dataframe\n",
        "\n",
        "    Returns:\n",
        "        outputdf: with two more columns. One is 'main_post': where 1=main post; 0 otherwise\n",
        "                                         The other is 'parent_author', gives the author name of the corresponding parent id\n",
        "    \"\"\"\n",
        "    main_list = []\n",
        "    parent_author_list = []\n",
        "\n",
        "    for i in range(outputdf.shape[0]):\n",
        "        uid = outputdf.parent_id[i].split('_')[-1]\n",
        "        ind = daydf[daydf['id'] == uid].index.to_list()\n",
        "        # print(ind)\n",
        "        if ind != []:\n",
        "            parent_author_list.append(daydf.loc[ind[0], 'author'])\n",
        "            if daydf.loc[ind[0], 'parent_id'] == daydf.loc[ind[0], 'link_id']:\n",
        "                main_list.append(1)\n",
        "            else:\n",
        "                main_list.append(0)\n",
        "        else:\n",
        "            main_list.append(np.nan)\n",
        "            parent_author_list.append(np.nan)\n",
        "            \n",
        "    outputdf['main_post'] =  main_list\n",
        "    outputdf['parent_author'] = parent_author_list\n",
        "    \n",
        "    return outputdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6Ei8C09PBax"
      },
      "outputs": [],
      "source": [
        "# A compact function that works out the OUTPUT dataframe\n",
        "def output_dataframe(daydf):\n",
        "    \"\"\"\n",
        "    A function that works out the OUTPUT dataframe from a DAY dataframe.\n",
        "    5 columns: parent_id, author, sub_comments, main_post and parent_author\n",
        "    Args:\n",
        "        daydf (): the DAY dataframe\n",
        "\n",
        "    Returns:\n",
        "        outputdf: the OUTPUT dataframe that is being reset index\n",
        "    \"\"\"\n",
        "    main_authors, outputdf = create_output_df(daydf)\n",
        "\n",
        "    outputdf = is_main(outputdf, daydf)\n",
        "\n",
        "    return main_authors, outputdf.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-------------------------------\n",
        "<h3>Create the adjacency matrix for a OUTPUT dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Auxilary functions to create the adj matrix\n",
        "\"\"\"\n",
        "def get_all_authors(name_id_day, output_df):\n",
        "    \"\"\"\n",
        "    A function to get all author names who created the post on this DAY from a OUTPUT dataframe \n",
        "    Args:\n",
        "        name_id_day (dictionary): a dictionary having the id as the key, its corresponding name as value from a DAY dataframe\n",
        "        output_df (pandas.dataframe): the output dataframe to be examined\n",
        "\n",
        "    Returns:\n",
        "        author_list: a list of author names\n",
        "    \"\"\"\n",
        "\n",
        "    author_list = set()\n",
        "    for i in range(output_df.shape[0]):\n",
        "        uid = output_df.parent_id.iloc[i].split('_')[-1]\n",
        "\n",
        "        try:\n",
        "            author_list.add(name_id_day[uid])\n",
        "        except:\n",
        "            # this would happend if author created the post on a posterior day\n",
        "            continue\n",
        "\n",
        "        author_list.update(tuple(output_df.author.iloc[i]))\n",
        "        \n",
        "    return author_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A compact function that returns the adj matrix\n",
        "def generate_adj_matrix(outputdf, name_id_day): \n",
        "    \"\"\"\n",
        "    A function to generate the adjacency matrix\n",
        "    Args:\n",
        "        outputdf (): the OUTPUT dataframe of the corresponding DAY dataframe\n",
        "        name_id_day (dictionary): a dictionary having the id as the key, its corresponding name as value from a DAY dataframe\n",
        "\n",
        "    Returns:\n",
        "        adj_matrix: the matrix\n",
        "    \"\"\"\n",
        "    # create the dict from the DAY dataframe\n",
        "    author_list = get_all_authors(name_id_day, outputdf)\n",
        "\n",
        "    adj_matrix = pd.DataFrame(0, index = author_list, columns = author_list)\n",
        "\n",
        "    for i in range(outputdf.shape[0]):\n",
        "        uid = outputdf.parent_id.iloc[i].split('_')[-1]\n",
        "        \n",
        "        try:\n",
        "            parent_name = name_id_day[uid]\n",
        "        except:\n",
        "            continue\n",
        "        author_list = outputdf.author.iloc[i]\n",
        "\n",
        "        for author in author_list:\n",
        "            adj_matrix.loc[author, parent_name] += 1\n",
        "    \n",
        "    return adj_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------\n",
        "<h3>Create an edge list from a DAY dataframe</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGJOxLYsJOmF"
      },
      "outputs": [],
      "source": [
        "def get_edge_list(outputdf, name_id_day, main_authors): \n",
        "    \"\"\"\n",
        "    A function to generate edge list dataframe\n",
        "    Args:\n",
        "        outputdf (): the OUTPUT dataframe\n",
        "        name_id_day (dictionary): a dictionary having the id as the key, its corresponding name as value from a DAY dataframe\n",
        "        main_authors (list): a list of authors the gives the main post\n",
        "\n",
        "    Returns:\n",
        "        edge_list: a dataframe of edge list with 3 colums, each are Source, Target and is_main\n",
        "    \"\"\"\n",
        "    source_list = []\n",
        "    target_list = []\n",
        "    is_main_list = []\n",
        "    for i in range(outputdf.shape[0]):\n",
        "        uid = outputdf.parent_id.iloc[i].split('_')[-1]        \n",
        "        try:\n",
        "            parent_name = name_id_day[uid]\n",
        "            main_post = 1 if parent_name in main_authors else  0 \n",
        "            author_list = outputdf.author.iloc[i]\n",
        "            for author in author_list:\n",
        "                source_list.append(author)\n",
        "                target_list.append(parent_name)\n",
        "                is_main_list.append(main_post)\n",
        "        except:\n",
        "            continue\n",
        "    edge_list = pd.DataFrame(list(zip(source_list, target_list, is_main_list)), columns = ['Source', 'Target', 'is_main'])\n",
        "\n",
        "    return edge_list\n",
        "\n",
        "def get_singleton(outputdf, daydf):\n",
        "    \"\"\"\n",
        "    A function that works out a list of authors recognised as singleton nodes in the edge list\n",
        "    Args:\n",
        "        outputdf (): the OUTPUT dataframe \n",
        "        daydf (): the DAY dataframe \n",
        "\n",
        "    Returns:\n",
        "        singleton: a list of author name that are singleton\n",
        "    \"\"\"\n",
        "    \n",
        "    complete_authors = daydf['author'].to_list()\n",
        "    non_single = []\n",
        "    for i in range(outputdf.shape[0]):\n",
        "        if outputdf.parent_author.iloc[i] == np.nan:\n",
        "            continue\n",
        "        else:\n",
        "            non_single.append(outputdf.parent_author.iloc[i])\n",
        "            non_single += outputdf.author.iloc[i]\n",
        "    non_single = list(dict.fromkeys(non_single))\n",
        "    complete_authors = list(dict.fromkeys(complete_authors))\n",
        "    singleton = set(complete_authors) - set(non_single)\n",
        "    \n",
        "    return singleton\n",
        "\n",
        "def singleton_edge(edge_list, singleton):\n",
        "    \"\"\"\n",
        "    A function adds singleton nodes to an existing edge list\n",
        "    Args:\n",
        "        edge_list (): the edge list\n",
        "        singleton (list):  a list of author name that are singleton\n",
        "\n",
        "    Returns:\n",
        "        edge_list: the updated edge list where SOURCE=TARGET=author from singleton and WEIGHT = 1\n",
        "    \"\"\"\n",
        "    \n",
        "    for author in singleton:\n",
        "        row = [author, author, 1]\n",
        "        edge_list.loc[len(edge_list.index)] = row\n",
        "    \n",
        "    return edge_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A compact function that returns two different edge list from an OUTPUT dataframe\n",
        "def e_lists(outputdf, daydf, main_authors, name_id_day):\n",
        "    standard_e = get_edge_list(outputdf, name_id_day, main_authors)\n",
        "\n",
        "    # to return the updated edge list\n",
        "    single = get_singleton(outputdf, daydf)\n",
        "\n",
        "    updated_e = standard_e[['Source', 'Target']]\n",
        "    updated_e = updated_e.groupby(updated_e.columns.tolist()).size().reset_index().rename(columns={0:'weights'})\n",
        "\n",
        "    updated_e = singleton_edge(updated_e, single)\n",
        "\n",
        "    return standard_e, updated_e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------------\n",
        "<h2>To walk throught a month</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a CHEAP month walker by now\n",
        "def month_walker(raw_df, year, month):\n",
        "    \"\"\"\n",
        "    A function returns a list of DAY information from a DAY dataframe inside a list of DAY-frames\n",
        "    The information has: 1. OUTPUT dataframe, 2. Adjacency matrix and 3. Two types of edge lists\n",
        "    Each information is of type tuple\n",
        "    Args:\n",
        "        raw_df (): dataframe not cleaned\n",
        "        year (int): the year\n",
        "        month (int): the month\n",
        "\n",
        "    Returns:\n",
        "        month_info: a list of information\n",
        "    \"\"\"\n",
        "    # clean dataframe\n",
        "    raw_df = raw_df.reset_index(drop=True)\n",
        "    cleaned_df = clean_df(raw_df)\n",
        "    # create a list of DAY dataframes\n",
        "    df_list = create_subframe(year, month, cleaned_df)\n",
        "\n",
        "    month_info = []\n",
        "\n",
        "    for daydf in df_list:\n",
        "        # this gives the name id dictionary for TODAY\n",
        "        name_id_day = create_name_id_dict(daydf)\n",
        "        # this gives you the OUTPUT dataframe\n",
        "        main_authors, outputdf = output_dataframe(daydf)\n",
        "        # this gives the adj matrix\n",
        "        adj_mat = generate_adj_matrix(outputdf, name_id_day)\n",
        "        # this gives the two edge lists\n",
        "        standard_e, updated_e = e_lists(outputdf, daydf, main_authors, name_id_day)\n",
        "\n",
        "        day_info = (outputdf, adj_mat, standard_e, updated_e)\n",
        "\n",
        "        month_info.append(day_info)\n",
        "    \n",
        "    return month_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fp1 = \"/home/vw/Desktop/UST_summer/01-20/01-20_comments.csv\"\n",
        "df_01 = pd.read_csv(fp1, low_memory = False, encoding = 'utf-8')  #get the Jan-2020 spreadsheet\n",
        "df_01 = df_01[df_01.author != '[deleted]'] #remove authors that are deleted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "month, year = 1, 2020\n",
        "raw_df = df_01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jan_2020_info = month_walker(raw_df,year,month)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLFVJqwNLUVH"
      },
      "outputs": [],
      "source": [
        "# def month_walker(month_df, path, mm_yy, save=True):\n",
        "#     if save:\n",
        "#         folder_path = os.path.join(path, mm_yy)\n",
        "#         os.makedirs(folder_path) \n",
        "#     for day_df in month_df:\n",
        "#         i = 1\n",
        "#         # first we create a pc_df and a dictionary to store the name\n",
        "#         main_authors, posts_authors = create_parent_children_df(day_df)\n",
        "#         name_id = create_name_id_dict(day_df)\n",
        "#         # then we clean pc_df by parent_id and its author column\n",
        "#         posts_authors = posts_authors.drop(clean_by_parent(posts_authors, name_id))\n",
        "#         posts_authors = clean_author(posts_authors)\n",
        "#         # finally create the adj matrix\n",
        "#         author_list = get_all_authors(name_id, posts_authors)\n",
        "#         source_list = []\n",
        "#         target_list = []\n",
        "#         is_main_list = []\n",
        "#         for i in range(dataframe.shape[0]):\n",
        "#           uid = dataframe.parent_id.iloc[i].split('_')[-1]        \n",
        "#           try:\n",
        "#             parent_name = name_id_dict[uid]\n",
        "#             main_post = 1 if parent_name in main_authors else  0 \n",
        "#             author_list = dataframe.author.iloc[i]\n",
        "#             for author in author_list:\n",
        "#               source_list.append(author)\n",
        "#               target_list.append(parent_name)\n",
        "#               is_main_list.append(main_post)\n",
        "#           except:\n",
        "#             continue\n",
        "#         edge_list = pd.DataFrame(list(zip(source_list, target_list, is_main_list)), columns = ['Source', 'Target', 'is_main'])\n",
        "#         # store the matrix as pickle\n",
        "#         save_path = os.path.join(folder_path, str(i))\n",
        "#         edge_list.to_pickle(save_path)\n",
        "#         i += 1\n",
        "    \n",
        "#     return matrices"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "reddits.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('ust')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b05a326980a2973dc75b11651072ceee1e3ced35f041c16412ae1ab4bd3b8488"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
